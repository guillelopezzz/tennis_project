{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tourney_id</th>\n",
       "      <th>tourney_name</th>\n",
       "      <th>surface</th>\n",
       "      <th>draw_size</th>\n",
       "      <th>tourney_level</th>\n",
       "      <th>tourney_date</th>\n",
       "      <th>match_num</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>winner_seed</th>\n",
       "      <th>winner_entry</th>\n",
       "      <th>...</th>\n",
       "      <th>l_1stIn</th>\n",
       "      <th>l_1stWon</th>\n",
       "      <th>l_2ndWon</th>\n",
       "      <th>l_SvGms</th>\n",
       "      <th>l_bpSaved</th>\n",
       "      <th>l_bpFaced</th>\n",
       "      <th>winner_rank</th>\n",
       "      <th>winner_rank_points</th>\n",
       "      <th>loser_rank</th>\n",
       "      <th>loser_rank_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-9900</td>\n",
       "      <td>United Cup</td>\n",
       "      <td>Hard</td>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>20230102</td>\n",
       "      <td>300</td>\n",
       "      <td>126203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-9900</td>\n",
       "      <td>United Cup</td>\n",
       "      <td>Hard</td>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>20230102</td>\n",
       "      <td>299</td>\n",
       "      <td>126207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-9900</td>\n",
       "      <td>United Cup</td>\n",
       "      <td>Hard</td>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>20230102</td>\n",
       "      <td>296</td>\n",
       "      <td>126203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2905.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-9900</td>\n",
       "      <td>United Cup</td>\n",
       "      <td>Hard</td>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>20230102</td>\n",
       "      <td>295</td>\n",
       "      <td>126207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-9900</td>\n",
       "      <td>United Cup</td>\n",
       "      <td>Hard</td>\n",
       "      <td>18</td>\n",
       "      <td>A</td>\n",
       "      <td>20230102</td>\n",
       "      <td>292</td>\n",
       "      <td>126774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2375.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  tourney_id tourney_name surface  draw_size tourney_level  tourney_date  \\\n",
       "0  2023-9900   United Cup    Hard         18             A      20230102   \n",
       "1  2023-9900   United Cup    Hard         18             A      20230102   \n",
       "2  2023-9900   United Cup    Hard         18             A      20230102   \n",
       "3  2023-9900   United Cup    Hard         18             A      20230102   \n",
       "4  2023-9900   United Cup    Hard         18             A      20230102   \n",
       "\n",
       "   match_num  winner_id  winner_seed winner_entry  ... l_1stIn l_1stWon  \\\n",
       "0        300     126203          3.0          NaN  ...    62.0     47.0   \n",
       "1        299     126207          NaN          NaN  ...    12.0      8.0   \n",
       "2        296     126203          3.0          NaN  ...    62.0     51.0   \n",
       "3        295     126207          NaN          NaN  ...    41.0     26.0   \n",
       "4        292     126774          1.0          NaN  ...    58.0     48.0   \n",
       "\n",
       "   l_2ndWon l_SvGms  l_bpSaved  l_bpFaced  winner_rank winner_rank_points  \\\n",
       "0      15.0    12.0        9.0        9.0          9.0             3355.0   \n",
       "1       3.0     4.0        1.0        3.0         19.0             2000.0   \n",
       "2       7.0    12.0        2.0        2.0          9.0             3355.0   \n",
       "3      12.0     9.0        6.0        9.0         19.0             2000.0   \n",
       "4      18.0    16.0        1.0        2.0          4.0             5550.0   \n",
       "\n",
       "  loser_rank loser_rank_points  \n",
       "0       16.0            2375.0  \n",
       "1       23.0            1865.0  \n",
       "2       10.0            2905.0  \n",
       "3      245.0             220.0  \n",
       "4       16.0            2375.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar el data set\n",
    "dataset = pd.read_csv('atp_matches_2023.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos el tratamiento de los NaN's antes de nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontramos el valor más común entre 'Grass', 'Hard', y 'Clay'\n",
    "most_common = dataset[dataset['surface'].isin(['Grass', 'Hard', 'Clay'])]['surface'].mode()[0]\n",
    "\n",
    "# Reemplazamos los valores que no son 'Grass', 'Hard', o 'Clay' (los NaN's) con el valor más común\n",
    "dataset.loc[~dataset['surface'].isin(['Grass', 'Hard', 'Clay']), 'surface'] = most_common\n",
    "\n",
    "# Reemplazamos los valores NaN de la columna de puntos en el ranking de los jugadores por 0\n",
    "dataset.iloc[:, [-1, -3]] = dataset.iloc[:, [-1, -3]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Indice 7: ganador del partido.\n",
    "Indice 15: perdedor del partido.\n",
    "Indice -1: puntos en el ranking del jugador ganador\n",
    "Indice -3: puntos en el ranking del jugador perdedor\n",
    "Indice 2: tipo de campo del partido (grass, hard o clay)'''\n",
    "X = dataset.iloc[:, [7, 15, -1, -3, 2]].values\n",
    "y = np.zeros((X.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 0] = X[:, 0].astype(str)\n",
    "X[:, 1] = X[:, 1].astype(str)\n",
    "# y = y.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el codificador\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Ajustar el codificador con los datos de ambos jugadores y 'y'\n",
    "encoder.fit(X[:, [0, 1]].reshape(-1, 1))\n",
    "\n",
    "# Transformar las columnas de los jugadores\n",
    "player1_onehot = encoder.transform(X[:, 0].reshape(-1, 1))\n",
    "player2_onehot = encoder.transform(X[:, 1].reshape(-1, 1))\n",
    "\n",
    "# Transformar el vector objetivo 'y' con el mismo codificador 'X'\n",
    "# y = encoder.transform(y.reshape(-1, 1))\n",
    "\n",
    "# Concatenar las columnas one-hot con el resto de tus datos\n",
    "X = np.concatenate([player1_onehot, player2_onehot, X[:, 2:]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de booleanos aleatorios del mismo tamaño que tus datos\n",
    "swap = np.random.rand(len(X)) > 0.5\n",
    "\n",
    "# Intercambiamos las posiciones de los jugadores en 'X' donde 'swap' es True\n",
    "X[swap, :440], X[swap, 440:880] = X[swap, 440:880], X[swap, :440].copy()\n",
    "\n",
    "# Intercambiamos las posiciones de los puntos en el ranking donde 'swap' es True\n",
    "X[swap, 880], X[swap, 881] = X[swap, 881], X[swap, 880].copy()\n",
    "y[swap] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el escalador\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Ajustamos y transformamos las dos últimas columnas de tus datos\n",
    "X[:, [880, 881]] = scaler.fit_transform(X[:, [880, 881]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es para el tipo de campo\n",
    "# Crear el codificador\n",
    "encoder2 = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Ajustar y transformar la columna del tipo de campo\n",
    "field_type_onehot = encoder2.fit_transform(X[:, 882].reshape(-1, 1))\n",
    "\n",
    "# Concatenar la columna one-hot con el resto de los datos\n",
    "X = np.concatenate([X[:, :882], field_type_onehot, X[:, 883:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.insert(X, 440, X[:, 880], axis=1)\n",
    "# Eliminar la columna 880\n",
    "X = np.delete(X, 881, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Divide el conjunto de entrenamiento en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "X_val = X_val.astype('float64')\n",
    "y_val = y_val.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 1s 4ms/step - loss: 0.6890 - accuracy: 0.5461 - val_loss: 0.6858 - val_accuracy: 0.5779\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6615 - accuracy: 0.7125 - val_loss: 0.6765 - val_accuracy: 0.5980\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.7437 - val_loss: 0.6680 - val_accuracy: 0.6080\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7627 - val_loss: 0.6725 - val_accuracy: 0.6030\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7828 - val_loss: 0.6791 - val_accuracy: 0.5980\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4990 - accuracy: 0.7917 - val_loss: 0.6939 - val_accuracy: 0.5946\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7956 - val_loss: 0.7044 - val_accuracy: 0.5997\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.8079 - val_loss: 0.7211 - val_accuracy: 0.6030\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.8202 - val_loss: 0.7414 - val_accuracy: 0.6013\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8275 - val_loss: 0.7469 - val_accuracy: 0.6064\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8370 - val_loss: 0.7588 - val_accuracy: 0.6064\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8420 - val_loss: 0.7753 - val_accuracy: 0.6030\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8476 - val_loss: 0.7942 - val_accuracy: 0.5946\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8492 - val_loss: 0.8016 - val_accuracy: 0.5930\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8626 - val_loss: 0.8085 - val_accuracy: 0.5963\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8666 - val_loss: 0.8235 - val_accuracy: 0.5930\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8772 - val_loss: 0.8438 - val_accuracy: 0.5879\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8811 - val_loss: 0.8541 - val_accuracy: 0.5879\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8861 - val_loss: 0.8642 - val_accuracy: 0.5879\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2790 - accuracy: 0.8973 - val_loss: 0.8716 - val_accuracy: 0.5812\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9028 - val_loss: 0.8867 - val_accuracy: 0.5930\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9101 - val_loss: 0.8975 - val_accuracy: 0.5879\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9174 - val_loss: 0.9094 - val_accuracy: 0.5930\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9202 - val_loss: 0.9222 - val_accuracy: 0.5913\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9296 - val_loss: 0.9268 - val_accuracy: 0.5896\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9330 - val_loss: 0.9404 - val_accuracy: 0.5930\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9375 - val_loss: 0.9512 - val_accuracy: 0.5980\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2008 - accuracy: 0.9453 - val_loss: 0.9771 - val_accuracy: 0.5980\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9470 - val_loss: 0.9803 - val_accuracy: 0.6064\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.9553 - val_loss: 0.9875 - val_accuracy: 0.5963\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1759 - accuracy: 0.9581 - val_loss: 1.0000 - val_accuracy: 0.5930\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1692 - accuracy: 0.9609 - val_loss: 1.0172 - val_accuracy: 0.5997\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9665 - val_loss: 1.0259 - val_accuracy: 0.5946\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 0.9682 - val_loss: 1.0351 - val_accuracy: 0.5963\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9743 - val_loss: 1.0475 - val_accuracy: 0.5980\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1395 - accuracy: 0.9738 - val_loss: 1.0601 - val_accuracy: 0.5997\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9810 - val_loss: 1.0768 - val_accuracy: 0.5980\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1276 - accuracy: 0.9793 - val_loss: 1.0787 - val_accuracy: 0.5963\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9810 - val_loss: 1.0981 - val_accuracy: 0.5963\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9816 - val_loss: 1.1071 - val_accuracy: 0.5980\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9838 - val_loss: 1.1240 - val_accuracy: 0.6013\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9866 - val_loss: 1.1319 - val_accuracy: 0.5930\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9855 - val_loss: 1.1486 - val_accuracy: 0.5963\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9872 - val_loss: 1.1529 - val_accuracy: 0.5946\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9872 - val_loss: 1.1707 - val_accuracy: 0.5930\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9877 - val_loss: 1.1834 - val_accuracy: 0.5946\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9894 - val_loss: 1.1850 - val_accuracy: 0.5946\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9894 - val_loss: 1.2075 - val_accuracy: 0.5963\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9905 - val_loss: 1.2253 - val_accuracy: 0.5980\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9916 - val_loss: 1.2322 - val_accuracy: 0.5946\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9894 - val_loss: 1.2413 - val_accuracy: 0.5930\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9888 - val_loss: 1.2583 - val_accuracy: 0.5879\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9883 - val_loss: 1.2681 - val_accuracy: 0.5930\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9894 - val_loss: 1.2856 - val_accuracy: 0.5930\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9894 - val_loss: 1.2906 - val_accuracy: 0.5879\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9888 - val_loss: 1.3033 - val_accuracy: 0.5913\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9888 - val_loss: 1.3107 - val_accuracy: 0.5896\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0567 - accuracy: 0.9899 - val_loss: 1.3244 - val_accuracy: 0.5930\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9883 - val_loss: 1.3326 - val_accuracy: 0.5963\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9894 - val_loss: 1.3422 - val_accuracy: 0.5946\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9866 - val_loss: 1.3603 - val_accuracy: 0.5896\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9899 - val_loss: 1.3772 - val_accuracy: 0.5863\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.9888 - val_loss: 1.3768 - val_accuracy: 0.5946\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9899 - val_loss: 1.3938 - val_accuracy: 0.5879\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9888 - val_loss: 1.4051 - val_accuracy: 0.5913\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9883 - val_loss: 1.4311 - val_accuracy: 0.5946\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9883 - val_loss: 1.4317 - val_accuracy: 0.5980\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9894 - val_loss: 1.4425 - val_accuracy: 0.5879\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9883 - val_loss: 1.4502 - val_accuracy: 0.5913\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9888 - val_loss: 1.4717 - val_accuracy: 0.5913\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9899 - val_loss: 1.4635 - val_accuracy: 0.5997\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 1.4826 - val_accuracy: 0.5879\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9888 - val_loss: 1.4912 - val_accuracy: 0.5896\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9916 - val_loss: 1.4894 - val_accuracy: 0.6013\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9860 - val_loss: 1.5108 - val_accuracy: 0.5930\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9899 - val_loss: 1.5232 - val_accuracy: 0.5863\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9911 - val_loss: 1.5353 - val_accuracy: 0.5796\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9905 - val_loss: 1.5534 - val_accuracy: 0.5913\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9883 - val_loss: 1.5413 - val_accuracy: 0.5963\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9899 - val_loss: 1.5522 - val_accuracy: 0.5930\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0310 - accuracy: 0.9872 - val_loss: 1.5654 - val_accuracy: 0.5913\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9888 - val_loss: 1.5702 - val_accuracy: 0.5946\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 1.5881 - val_accuracy: 0.5963\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.9894 - val_loss: 1.5914 - val_accuracy: 0.5913\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9905 - val_loss: 1.6016 - val_accuracy: 0.5879\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9883 - val_loss: 1.6124 - val_accuracy: 0.5946\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9888 - val_loss: 1.6198 - val_accuracy: 0.5913\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9888 - val_loss: 1.6247 - val_accuracy: 0.5963\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9899 - val_loss: 1.6477 - val_accuracy: 0.5930\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9894 - val_loss: 1.6636 - val_accuracy: 0.5879\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9894 - val_loss: 1.6699 - val_accuracy: 0.5896\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9894 - val_loss: 1.6807 - val_accuracy: 0.5846\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9894 - val_loss: 1.6723 - val_accuracy: 0.5879\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.9899 - val_loss: 1.6725 - val_accuracy: 0.5980\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 1.6891 - val_accuracy: 0.5930\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9899 - val_loss: 1.7029 - val_accuracy: 0.5896\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9888 - val_loss: 1.7130 - val_accuracy: 0.5913\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9894 - val_loss: 1.7043 - val_accuracy: 0.5930\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9888 - val_loss: 1.7248 - val_accuracy: 0.5946\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9888 - val_loss: 1.7336 - val_accuracy: 0.5913\n"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Añadir la capa de entrada y la primera capa oculta\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "# Añadir la segunda capa oculta\n",
    "# model.add(Dense(units=32, activation='tanh'))\n",
    "\n",
    "# Añadir la capa de salida\n",
    "model.add(Dense(units=y.shape[1], activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float64')\n",
    "y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 778us/step\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "predicciones = model.predict(X_test)\n",
    "\n",
    "# Como estás haciendo una clasificación binaria y usaste una función de activación sigmoide en tu capa de salida,\n",
    "# 'predicciones' será una matriz de probabilidades. Puedes convertir estas probabilidades en clases binarias (0 o 1) así:\n",
    "predicciones_binarias = (predicciones > 0.5).astype(int)\n",
    "print(predicciones_binarias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Usamos el codificador para transformar las etiquetas codificadas a sus etiquetas originales\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del test: 58.7%\n"
     ]
    }
   ],
   "source": [
    "# Veamos cuántos partidos ha acertado para el test\n",
    "aciertos = 0\n",
    "for i in range(len(y_test)):\n",
    "    if predicciones_binarias[i] == y_test[i]:\n",
    "        aciertos += 1\n",
    "print(f\"Precisión del test: {np.round(aciertos / len(y_test)*100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
